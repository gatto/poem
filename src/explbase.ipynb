{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study of the explanation base size\n",
    "## Run these once, at the start of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from time import perf_counter\n",
    "\n",
    "import numpy as np\n",
    "import oab\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make results list\n",
    "results_table = []\n",
    "domain_loading_times = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personalize the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# personalize this for running the notebook in different ways\n",
    "dataset_name = \"mnist\"\n",
    "algo_name = \"RF\"\n",
    "\n",
    "how_many_images = 50  # how many images each combination of the above?\n",
    "size = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run these after each \"personalization\"\n",
    "To get results on multiple datasets, algorithms, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cells *not* timed\n",
    "Data loading and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test), (X_tree, Y_tree) = oab.get_data(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timed cells\n",
    "Domain loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = perf_counter()\n",
    "my_dom = oab.Domain(dataset_name, algo_name, subset_size=size)\n",
    "end = perf_counter()\n",
    "execution_time = end - start\n",
    "\n",
    "# append into results_table\n",
    "domain_loading_times.append(\n",
    "    {\n",
    "        \"Dataset\": dataset_name,\n",
    "        \"Algo\": algo_name,\n",
    "        \"Expl-base size\": size,\n",
    "        \"Time\": round(execution_time, 2),\n",
    "    }\n",
    ")\n",
    "\n",
    "index = {}\n",
    "points = {}\n",
    "for my_class in range(10):\n",
    "    index[my_class] = np.where(Y_test == my_class)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actual explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for my_class in [int(x) for x in my_dom.classes]:\n",
    "    for i, my_index in enumerate(index[my_class]):\n",
    "        start = perf_counter()\n",
    "        testpoint = oab.TestPoint(X_test[my_index], my_dom)\n",
    "        logging.info(f\"start explanation of index {my_index}\")\n",
    "        exp = oab.Explainer(testpoint, howmany=5)\n",
    "\n",
    "        end = perf_counter()\n",
    "        execution_time = end - start\n",
    "\n",
    "        # if we failed to find a target\n",
    "        if exp.target:\n",
    "            crules = len(exp.target.latentdt.counterrules)\n",
    "        else:\n",
    "            crules = \"error\"\n",
    "\n",
    "        # append into results_table\n",
    "        results_table.append(\n",
    "            {\n",
    "                \"Dataset\": dataset_name,\n",
    "                \"Algo\": algo_name,\n",
    "                \"Expl-base size\": size,\n",
    "                \"Class\": my_class,\n",
    "                \"id\": my_index,\n",
    "                \"Time\": round(execution_time, 2),\n",
    "                \"knn extra runs\": exp.knn_runs_count,\n",
    "                \"factuals / 5\": len(exp.factuals),\n",
    "                \"cfact\": len(exp.counterfactuals),\n",
    "                \"crules\": crules,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if i >= how_many_images - 1:\n",
    "            # how many points to do per class\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this to show the results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_dataframe = pd.DataFrame.from_records(domain_loading_times)\n",
    "domain_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dataframe = pd.DataFrame.from_records(results_table)\n",
    "results_dataframe"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
