{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rigidness of polyhedron\n",
    "\n",
    "Let's check the effect on execution time of how rigid the hyperpolyhedron check is. Let's compare the results we already got (in time.ipynb) with these results, which execute the explanations with flexible polyhedron margins.\n",
    "\n",
    "## Run these once, at the start of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from time import perf_counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rich import print\n",
    "\n",
    "import oab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make results list\n",
    "results_table = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personalize the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# personalize this for running the notebook in different ways\n",
    "dataset_name = \"mnist\"\n",
    "algo_name = \"RF\"\n",
    "\n",
    "how_many_images = 50  # how many images each combination of the above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run these after each \"personalization\"\n",
    "To get results on multiple datasets, algorithms, classes\n",
    "### Cells *not* timed\n",
    "Data loading and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test), (X_tree, Y_tree) = oab.get_data(dataset_name)\n",
    "my_dom = oab.Domain(dataset_name, algo_name)\n",
    "index = {}\n",
    "points = {}\n",
    "for my_class in range(10):\n",
    "    index[my_class] = np.where(Y_test == my_class)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timed cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for my_class in [int(x) for x in my_dom.classes]:\n",
    "    for i, my_index in enumerate(index[my_class]):\n",
    "        start = perf_counter()\n",
    "        testpoint = oab.TestPoint(X_test[my_index], my_dom)\n",
    "        logging.info(f\"start explanation of index {my_index}\")\n",
    "        exp = oab.Explainer(\n",
    "            testpoint, howmany=5, margins=\"soft\"\n",
    "        )  # here is the margins rigidness\n",
    "\n",
    "        end = perf_counter()\n",
    "        execution_time = end - start\n",
    "\n",
    "        # if we failed to find a target\n",
    "        if exp.target:\n",
    "            crules = len(exp.target.latentdt.counterrules)\n",
    "        else:\n",
    "            crules = \"error\"\n",
    "\n",
    "        # append into results_table\n",
    "        results_table.append(\n",
    "            {\n",
    "                \"Dataset\": dataset_name,\n",
    "                \"Algo\": algo_name,\n",
    "                \"Class\": my_class,\n",
    "                \"id\": my_index,\n",
    "                \"Time\": round(execution_time, 2),\n",
    "                \"factuals / 5\": len(exp.factuals),\n",
    "                \"cfact\": len(exp.counterfactuals),\n",
    "                \"crules\": crules,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        if i >= how_many_images - 1:\n",
    "            # how many points to do per class\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this to show the results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dataframe = pd.DataFrame.from_records(results_table)\n",
    "results_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the **factuals generated are less than 5**, it means out of 100 generated (and discriminated) candidate factuals, less than 5 actually predicted to the same class as the point being explained. Same thing if the factuals generated are 0: out of 100 generated and discriminated images, none predicted to the correct class.\n",
    "\n",
    "If the **counterfactuals generated are less than the counterrules**, it means one of the following:\n",
    "1. one or more of those counterfactuals did not pass the discriminator or\n",
    "2. one or more did not predict to a different class than the point being explained.\n",
    "\n",
    "If **in the crules column appears \"error\"**, it means that the KNN was not able to find a target TreePoint in our explanation base for the failure of our 3 checks. Therefore, we fail for the entire explanation process (factuals and counterfactuals generated are zero).\n",
    "\n",
    "If the **crules (counterrules) generated are 0**, it's an Abele failure and I can't generate any counterexemplars."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
