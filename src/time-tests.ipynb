{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timing of oab\n",
    "## Run these once, at the start of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from time import perf_counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rich import print\n",
    "\n",
    "import oab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make results list\n",
    "results_table = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Personalize the following cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# personalize this for running the notebook in different ways\n",
    "algo_name = \"RF\"\n",
    "\n",
    "how_many_images = 3  # how many images each combination of the above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run these after each \"personalization\"\n",
    "To get results on multiple datasets, algorithms, classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timed cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"mnist\", \"fashion\", \"emnist\"]\n",
    "\n",
    "for dataset_name in datasets:\n",
    "    (X_train, Y_train), (X_test, Y_test), (X_tree, Y_tree) = oab.get_data(dataset_name)\n",
    "    my_dom = oab.Domain(dataset_name, algo_name)\n",
    "    index = {}\n",
    "    points = {}\n",
    "    for my_class in range(10):\n",
    "        index[my_class] = np.where(Y_test == my_class)[0]\n",
    "\n",
    "    for my_class in [int(x) for x in my_dom.classes]:\n",
    "        for i, my_index in enumerate(index[my_class]):\n",
    "            start = perf_counter()\n",
    "            testpoint = oab.TestPoint(X_test[my_index], my_dom)\n",
    "            logging.info(f\"start explanation of index {my_index}\")\n",
    "            exp = oab.Explainer(testpoint, howmany=5)\n",
    "\n",
    "            if not exp.factuals:\n",
    "                exp.factuals = []\n",
    "            if not exp.counterfactuals:\n",
    "                exp.counterfactuals = []\n",
    "\n",
    "            end = perf_counter()\n",
    "            execution_time = end - start\n",
    "\n",
    "            # if we failed to find a target\n",
    "            if exp.target:\n",
    "                crules = len(exp.target.latentdt.counterrules)\n",
    "            else:\n",
    "                crules = \"error\"\n",
    "\n",
    "            # append into results_table\n",
    "            results_table.append(\n",
    "                {\n",
    "                    \"Dataset\": dataset_name,\n",
    "                    \"Algo\": algo_name,\n",
    "                    \"Class\": my_class,\n",
    "                    \"id\": my_index,\n",
    "                    \"Time\": execution_time,\n",
    "                    \"factuals / 5\": len(exp.factuals),\n",
    "                    \"cfact\": len(exp.counterfactuals),\n",
    "                    \"crules\": crules,\n",
    "                }\n",
    "            )\n",
    "\n",
    "            if i >= how_many_images - 1:\n",
    "                # how many points to do per class\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run this to show the results table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dataframe = pd.DataFrame.from_records(results_table)\n",
    "results_dataframe[\"Time\"] = results_dataframe[\"Time\"].round(2)\n",
    "results_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dom = oab.Domain(\"fashion\", \"DNN\")\n",
    "print(f\"{len(my_dom.explanation_base)=}\")\n",
    "i = 0\n",
    "for row in my_dom.explanation_base:\n",
    "    if len(row) != 4:\n",
    "        i += 1\n",
    "print(f\"oh well {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the **factuals generated are less than 5**, it means out of 100 generated (and discriminated) candidate factuals, less than 5 actually predicted to the same class as the point being explained. Same thing if the factuals generated are 0: out of 100 generated and discriminated images, none predicted to the correct class.\n",
    "\n",
    "If the **counterfactuals generated are less than the counterrules**, it means one of the following:\n",
    "1. one or more of those counterfactuals did not pass the discriminator or\n",
    "2. one or more did not predict to a different class than the point being explained.\n",
    "\n",
    "If **in the crules column appears \"error\"**, it means that the KNN was not able to find a target TreePoint in our explanation base for the failure of our 3 checks. Therefore, we fail for the entire explanation process (factuals and counterfactuals generated are zero).\n",
    "\n",
    "If the **crules (counterrules) generated are 0**, it's an Abele failure and I can't generate any counterexemplars."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
